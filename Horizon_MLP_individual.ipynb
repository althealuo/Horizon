{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49a2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb9d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.data_processing as data_processing\n",
    "importlib.reload(data_processing)\n",
    "from utils.data_processing import set_seed, save_output, get_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe9dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e4c2c",
   "metadata": {},
   "source": [
    "# data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c3ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7699168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Data (X) ---\n",
      "     r0   c0    r1   c1    r2   c2    r3   c3  gameLength  uc  ...  subj_990  \\\n",
      "0  42.0  0.0  45.0  1.0  42.0  1.0  18.0  1.0           6   1  ...     False   \n",
      "1  67.0  0.0  57.0  1.0  56.0  0.0  50.0  1.0           6   0  ...     False   \n",
      "2  37.0  1.0  48.0  0.0  23.0  0.0  39.0  1.0           6   0  ...     False   \n",
      "3  58.0  1.0  51.0  0.0  28.0  0.0  47.0  1.0           1   0  ...     False   \n",
      "4   4.0  1.0  30.0  0.0  11.0  1.0  37.0  0.0           1   0  ...     False   \n",
      "\n",
      "   subj_991  subj_992  subj_993  subj_994  subj_995  subj_996  subj_997  \\\n",
      "0     False     False     False     False     False     False     False   \n",
      "1     False     False     False     False     False     False     False   \n",
      "2     False     False     False     False     False     False     False   \n",
      "3     False     False     False     False     False     False     False   \n",
      "4     False     False     False     False     False     False     False   \n",
      "\n",
      "   subj_998  subj_999  \n",
      "0     False     False  \n",
      "1     False     False  \n",
      "2     False     False  \n",
      "3     False     False  \n",
      "4     False     False  \n",
      "\n",
      "[5 rows x 1394 columns]\n",
      "\n",
      "--- Target Data (y) ---\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    1.0\n",
      "4    0.0\n",
      "Name: c4, dtype: float64\n",
      "\n",
      "Train set: (136652, 1394), Test set: (34333, 1394)\n",
      "H1 samples: 17170, H6 samples: 17163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Define feature groups ---\n",
    "seq_features = ['r0', 'c0', 'r1', 'c1', 'r2', 'c2', 'r3', 'c3']\n",
    "static_features = ['gameLength', 'uc']\n",
    "target = 'c4'\n",
    "\n",
    "# --- One-hot encode subjects ---\n",
    "subj_onehot = pd.get_dummies(df['subject'], prefix='subj')\n",
    "df_static = pd.concat([df[static_features], subj_onehot], axis=1)\n",
    "\n",
    "# --- Combine all features ---\n",
    "X = pd.concat([df[seq_features], df_static], axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# --- Split by subjects (to ensure unseen subjects in test) ---\n",
    "subjects = df['subject'].unique()\n",
    "train_subj, test_subj = train_test_split(subjects, test_size=0.2, random_state=42)\n",
    "\n",
    "train_mask = df['subject'].isin(train_subj)\n",
    "test_mask  = df['subject'].isin(test_subj)\n",
    "\n",
    "X_train = X.loc[train_mask]\n",
    "X_test  = X.loc[test_mask]\n",
    "y_train = y.loc[train_mask]\n",
    "y_test  = y.loc[test_mask]\n",
    "\n",
    "# --- Separate numeric vs one-hot for scaling ---\n",
    "onehot_cols = [c for c in X.columns if c.startswith('subj_')]\n",
    "numeric_cols = seq_features + static_features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_num  = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "X_train_oh = X_train[onehot_cols].to_numpy()\n",
    "X_test_oh  = X_test[onehot_cols].to_numpy()\n",
    "\n",
    "# --- Combine back numeric + one-hot ---\n",
    "X_train = np.hstack([X_train_num, X_train_oh])\n",
    "X_test  = np.hstack([X_test_num, X_test_oh])\n",
    "\n",
    "# --- Optional: track horizon masks for evaluation ---\n",
    "h1_mask = X.loc[test_mask, 'gameLength'] == 1\n",
    "h6_mask = X.loc[test_mask, 'gameLength'] == 6\n",
    "\n",
    "print(\"--- Feature Data (X) ---\")\n",
    "print(X.head())\n",
    "print(\"\\n--- Target Data (y) ---\")\n",
    "print(y.head())\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "print(f\"H1 samples: {h1_mask.sum()}, H6 samples: {h6_mask.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4137c6",
   "metadata": {},
   "source": [
    "convert to tensor for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35eebbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129e6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32) # sklearn output float64, doesn't work with torch\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32) \n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long) # pandas series to tensor\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddaabfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acfbc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the scaled tensor for h1 h6\n",
    "# convert pandas series -> numpy array -> torch BoolTensor\n",
    "h1_mask_bool = torch.tensor(h1_mask.to_numpy(), dtype=torch.bool)\n",
    "h6_mask_bool = torch.tensor(h6_mask.to_numpy(), dtype=torch.bool)\n",
    "\n",
    "X_test_h1 = X_test_tensor[h1_mask_bool]\n",
    "X_test_h6 = X_test_tensor[h6_mask_bool]\n",
    "\n",
    "y_test_h1 = y_test_tensor[h1_mask_bool]\n",
    "y_test_h6 = y_test_tensor[h6_mask_bool]\n",
    "\n",
    "test_loader_h1 = DataLoader(TensorDataset(X_test_h1, y_test_h1), batch_size=32, shuffle=False)\n",
    "test_loader_h6 = DataLoader(TensorDataset(X_test_h6, y_test_h6), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c78745",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "891d60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import (\n",
    "    MLP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd32b8",
   "metadata": {},
   "source": [
    "# train / eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139239ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device): \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = len(train_loader)\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = train_loss / total\n",
    "    accuracy = correct / total\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    total = len(test_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f45961",
   "metadata": {},
   "source": [
    "# running experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c0ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = len(np.unique(y_train))  # usually 2 for left/right choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea099be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"MLP\": MLP(input_size=input_size, output_size=output_size).to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aa91c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(model, train_loader, test_loaders, criterion, optimizer, device, epochs):\n",
    "    test_loader, test_loader_h1, test_loader_h6 = test_loaders\n",
    "    train_loss_prog, train_acc_prog = [], []\n",
    "\n",
    "    test_loss_prog, test_acc_prog = [], []\n",
    "\n",
    "    test_acc_h1_prog, test_loss_h1_prog = [], []\n",
    "    test_acc_h6_prog, test_loss_h6_prog = [], []\n",
    "\n",
    "    epochs_without_improvement = 0 # for early stopping\n",
    "    best_loss = float('inf')\n",
    "    PATIENCE = 5\n",
    "    final_epoch = epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_acc, train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        test_acc, test_loss = test(model, test_loader, criterion, device)\n",
    "        test_acc_h1, test_loss_h1 = test(model, test_loader_h1, criterion, device)\n",
    "        test_acc_h6, test_loss_h6 = test(model, test_loader_h6, criterion, device)\n",
    "        if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss: {test_loss:.4f} | overall: {test_acc:.4f} | H1 {test_acc_h1:.4f} | H6 {test_acc_h6:.4f}\")\n",
    "\n",
    "        train_acc_prog.append(train_acc)\n",
    "        train_loss_prog.append(train_loss)\n",
    "\n",
    "        test_loss_prog.append(test_loss)\n",
    "        test_acc_prog.append(test_acc)\n",
    "\n",
    "        test_acc_h1_prog.append(test_acc_h1)\n",
    "        test_loss_h1_prog.append(test_loss_h1)\n",
    "\n",
    "        test_acc_h6_prog.append(test_acc_h6)\n",
    "        test_loss_h6_prog.append(test_loss_h6)\n",
    "\n",
    "        # early stopping\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement > PATIENCE: \n",
    "            print(f\"Early stopping triggered: epoch {epoch+1} best_loss {best_loss:.4f}\")\n",
    "            final_epoch = epoch+1\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"train_loss_prog\": train_loss_prog,\n",
    "        \"train_acc_prog\": train_acc_prog,\n",
    "        \"test_loss_prog\": test_loss_prog,\n",
    "        \"test_acc_prog\": test_acc_prog,\n",
    "        \"test_acc_h1_prog\": test_acc_h1_prog,\n",
    "        \"test_loss_h1_prog\": test_loss_h1_prog,\n",
    "        \"test_acc_h6_prog\": test_acc_h6_prog,\n",
    "        \"test_loss_h6_prog\": test_loss_h6_prog,\n",
    "        \"final_epoch\": final_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6313212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: MLP\n",
      "Epoch 1: Loss: 0.5078 | overall: 0.7719 | H1 0.8126 | H6 0.7312\n",
      "Early stopping triggered: epoch 8 best_loss 0.5062\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "test_loaders = (test_loader, test_loader_h1, test_loader_h6)\n",
    "for model_name, model in model_dict.items():\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    history = train_and_evaluate(model, train_loader, test_loaders, criterion, optimizer, device, epochs=epochs)\n",
    "    model_dict[model_name] = {\n",
    "        \"model\": model,\n",
    "        **history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b30425d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model results to output_mlp_individual.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_output(model_dict, \"output_mlp_individual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faedf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
