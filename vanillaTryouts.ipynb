{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe7cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74f6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/allHorizonData_cut.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff420dbb",
   "metadata": {},
   "source": [
    "# data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d91e1",
   "metadata": {},
   "source": [
    "base data\n",
    "- based on `gameLength` and `uc` \n",
    "- predict `c5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dea0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df[df[\"c5\"].notna()][[\"gameLength\", \"uc\", \"c5\"]].dropna()\n",
    "\n",
    "# One-hot encode gameLength and uc\n",
    "X = pd.get_dummies(df_filtered[[\"gameLength\", \"uc\"]], columns=[\"gameLength\", \"uc\"])\n",
    "y = df_filtered[\"c5\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801ce80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gameLength_5  gameLength_10   uc_1   uc_2   uc_3\n",
      "0          True          False  False  False   True\n",
      "1         False           True  False  False   True\n",
      "2         False           True  False   True  False\n",
      "3         False           True  False   True  False\n",
      "4         False           True  False   True  False\n"
     ]
    }
   ],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61d8759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    2\n",
      "Name: c5, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60569d7",
   "metadata": {},
   "source": [
    "additional info\n",
    "- `c1-5` `r1-4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82bc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_features = ['gameLength', 'uc', 'r1', 'r2', 'r3', 'r4', 'c1', 'c2', 'c3', 'c4', 'c5']\n",
    "df_extended = df[extended_features].dropna()\n",
    "\n",
    "X_ext = pd.get_dummies(df_extended.drop(columns=\"c5\"), columns=[\"gameLength\", \"uc\", \"c1\", \"c2\", \"c3\", \"c4\"])\n",
    "y = df_extended[\"c5\"]\n",
    "\n",
    "X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(X_ext, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e350cec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   r1  r2  r3  r4  gameLength_5  gameLength_10   uc_1   uc_2   uc_3   c1_1  \\\n",
      "0  66  80  29  75          True          False  False  False   True  False   \n",
      "1  69  50  51  64         False           True  False  False   True  False   \n",
      "2  31  43  26  36         False           True  False   True  False  False   \n",
      "3  65  77  52  73         False           True  False   True  False   True   \n",
      "4  70  19  43  41         False           True  False   True  False  False   \n",
      "\n",
      "    c1_2   c2_1   c2_2   c3_1   c3_2   c4_1   c4_2  \n",
      "0   True  False   True   True  False  False   True  \n",
      "1   True  False   True   True  False  False   True  \n",
      "2   True   True  False  False   True   True  False  \n",
      "3  False  False   True  False   True   True  False  \n",
      "4   True   True  False  False   True   True  False  \n"
     ]
    }
   ],
   "source": [
    "print(X_ext.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694607d",
   "metadata": {},
   "source": [
    "# vanilla logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f702ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.53      0.53      1941\n",
      "           2       0.52      0.53      0.53      1899\n",
      "\n",
      "    accuracy                           0.53      3840\n",
      "   macro avg       0.53      0.53      0.53      3840\n",
      "weighted avg       0.53      0.53      0.53      3840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "report_log = classification_report(y_test, y_pred_log)\n",
    "print(report_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e7e01ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.49      0.52      1941\n",
      "           2       0.53      0.58      0.55      1899\n",
      "\n",
      "    accuracy                           0.54      3840\n",
      "   macro avg       0.54      0.54      0.54      3840\n",
      "weighted avg       0.54      0.54      0.54      3840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_model_ext = LogisticRegression(max_iter=1000)\n",
    "log_model_ext.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "y_pred_ext = log_model_ext.predict(X_test_ext)\n",
    "report_ext = classification_report(y_test_ext, y_pred_ext)\n",
    "print(report_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb6e52",
   "metadata": {},
   "source": [
    "# dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d7ae08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae37bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.64      0.58      1941\n",
      "           2       0.52      0.40      0.45      1899\n",
      "\n",
      "    accuracy                           0.52      3840\n",
      "   macro avg       0.52      0.52      0.51      3840\n",
      "weighted avg       0.52      0.52      0.51      3840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dtree_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dtree_base = dtree_model.predict(X_test)\n",
    "report_dtree_base = classification_report(y_test, y_pred_dtree_base)\n",
    "print(report_dtree_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33eec261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.68      0.70      1941\n",
      "           2       0.69      0.73      0.71      1899\n",
      "\n",
      "    accuracy                           0.70      3840\n",
      "   macro avg       0.70      0.70      0.70      3840\n",
      "weighted avg       0.70      0.70      0.70      3840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtree_ext_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dtree_ext_model.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "y_pred_dtree_ext = dtree_ext_model.predict(X_test_ext)\n",
    "report_dtree_ext = classification_report(y_test_ext, y_pred_dtree_ext)\n",
    "print(report_dtree_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52ff65",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7215187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71d0b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.40      0.46      1941\n",
      "           2       0.52      0.65      0.58      1899\n",
      "\n",
      "    accuracy                           0.53      3840\n",
      "   macro avg       0.53      0.53      0.52      3840\n",
      "weighted avg       0.53      0.53      0.52      3840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf_base_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_base_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf_base = rf_base_model.predict(X_test)\n",
    "report_rf_base = classification_report(y_test, y_pred_rf_base)\n",
    "print(report_rf_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba8ea121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.71      0.72      1941\n",
      "           2       0.71      0.74      0.72      1899\n",
      "\n",
      "    accuracy                           0.72      3840\n",
      "   macro avg       0.72      0.72      0.72      3840\n",
      "weighted avg       0.72      0.72      0.72      3840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf_ext_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_ext_model.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "y_pred_rf_ext = rf_ext_model.predict(X_test_ext)\n",
    "report_rf_ext = classification_report(y_test_ext, y_pred_rf_ext)\n",
    "print(report_rf_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419832e",
   "metadata": {},
   "source": [
    "# paper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fab7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_r (reward difference: left - right)\n",
    "def compute_delta_r(row):\n",
    "    left_rewards = [row[f\"r{i}\"] for i in range(1, 5) if row[f\"c{i}\"] == 1]\n",
    "    right_rewards = [row[f\"r{i}\"] for i in range(1, 5) if row[f\"c{i}\"] == 2]\n",
    "    return (sum(left_rewards)/len(left_rewards) if left_rewards else 0) - \\\n",
    "           (sum(right_rewards)/len(right_rewards) if right_rewards else 0)\n",
    "\n",
    "# Compute delta_i (information difference: left - right sample count)\n",
    "def compute_delta_i(row):\n",
    "    left_count = sum(1 for i in range(1, 5) if row[f\"c{i}\"] == 1)\n",
    "    right_count = sum(1 for i in range(1, 5) if row[f\"c{i}\"] == 2)\n",
    "    return left_count - right_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_needed = ['gameLength', 'uc', 'r1', 'r2', 'r3', 'r4', 'c1', 'c2', 'c3', 'c4', 'c5']\n",
    "df_c5 = df[columns_needed].dropna()\n",
    "\n",
    "df_c5[\"delta_r\"] = df_c5.apply(compute_delta_r, axis=1)\n",
    "df_c5[\"delta_i\"] = df_c5.apply(compute_delta_i, axis=1)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df_c5[[\"delta_r\", \"delta_i\"]]\n",
    "y = df_c5[\"c5\"]\n",
    "\n",
    "X_train_ext_p, X_test_ext_p, y_train_ext_p, y_test_ext_p = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62bd9d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.53      0.53      1941\n",
      "           2       0.52      0.53      0.53      1899\n",
      "\n",
      "    accuracy                           0.53      3840\n",
      "   macro avg       0.53      0.53      0.53      3840\n",
      "weighted avg       0.53      0.53      0.53      3840\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'delta_r': np.float64(-0.024253646926063868),\n",
       "  'delta_i': np.float64(0.04804338653314345)},\n",
       " np.float64(0.02384945013116338))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract coefficients\n",
    "coefficients = dict(zip(X.columns, model.coef_[0]))\n",
    "intercept = model.intercept_[0]\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "coefficients, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "408a2a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'delta_r': np.float64(-0.11416295589455795),\n",
       "  'delta_i': np.float64(-0.06049586493337527)},\n",
       " np.float64(0.00645967913705073),\n",
       " {'delta_r': np.float64(-0.06162044365071676),\n",
       "  'delta_i': np.float64(0.2237592231749833)},\n",
       " np.float64(0.08941557304833342))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into Horizon 1 and Horizon 6 groups\n",
    "df_c5_h1 = df_c5[df_c5[\"gameLength\"] == 5]\n",
    "df_c5_h6 = df_c5[df_c5[\"gameLength\"] == 10]\n",
    "\n",
    "# Prepare inputs for each group\n",
    "X_h1 = df_c5_h1[[\"delta_r\", \"delta_i\"]]\n",
    "y_h1 = df_c5_h1[\"c5\"]\n",
    "\n",
    "X_h6 = df_c5_h6[[\"delta_r\", \"delta_i\"]]\n",
    "y_h6 = df_c5_h6[\"c5\"]\n",
    "\n",
    "# Train logistic regression for Horizon 1\n",
    "model_h1 = LogisticRegression()\n",
    "model_h1.fit(X_h1, y_h1)\n",
    "alpha_beta_h1 = dict(zip(X_h1.columns, model_h1.coef_[0]))\n",
    "intercept_h1 = model_h1.intercept_[0]\n",
    "\n",
    "# Train logistic regression for Horizon 6\n",
    "model_h6 = LogisticRegression()\n",
    "model_h6.fit(X_h6, y_h6)\n",
    "alpha_beta_h6 = dict(zip(X_h6.columns, model_h6.coef_[0]))\n",
    "intercept_h6 = model_h6.intercept_[0]\n",
    "\n",
    "alpha_beta_h1, intercept_h1, alpha_beta_h6, intercept_h6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b721c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8465625, 0.7335416666666666)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict and evaluate for Horizon 1\n",
    "y_pred_h1 = model_h1.predict(X_h1)\n",
    "accuracy_h1 = accuracy_score(y_h1, y_pred_h1)\n",
    "\n",
    "# Predict and evaluate for Horizon 6\n",
    "y_pred_h6 = model_h6.predict(X_h6)\n",
    "accuracy_h6 = accuracy_score(y_h6, y_pred_h6)\n",
    "\n",
    "accuracy_h1, accuracy_h6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9b637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
