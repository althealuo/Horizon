{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aae7bb3-4d48-47d0-abd4-2b41d3ac5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53aa2ec6-10b4-4c3c-9d3f-3994e23ab804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e349e669-e11d-433f-a3a7-9d32a3e7695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import snntorch.spikeplot as splt\n",
    "# from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "393d5850-4dd8-460d-ae49-16a735df11fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92af65ba-0a5b-47b9-be41-15ae757b17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = Path('SEED')\n",
    "EMOTIONS = {\n",
    "    'Neutral': 0,\n",
    "    'Happy': 1,\n",
    "    'Sad': 2,\n",
    "    'Anger': 3,\n",
    "    'Fear': 4,\n",
    "    'Disgust': 5,\n",
    "    'Surprise': 6\n",
    "}\n",
    "TRAIN_SPLIT = 0.8\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "LAYER_COUNT = 4\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "GRADIENT_CLIPPING_MAX_NORM = 0.5\n",
    "MAX_EPOCHS = 2000\n",
    "EARLY_STOPPING_PATIENCE = 350\n",
    "PRINT_FREQUENCY_EPOCHS = 10\n",
    "\n",
    "\n",
    "NUM_STEPS = 10  # Adjust for poisson temporal depth\n",
    "FIXED_SEQ_LEN = 70\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48ebd78-32e5-4615-8ec0-a9f4d3f64d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeedDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.features = []\n",
    "        for subject_index in range(1, 21):\n",
    "            subject_features = sio.loadmat(str(DATASET_PATH / 'EEG_features'\n",
    "                                               / f'{subject_index}.mat'))\n",
    "            for video_index in range(1, 81):\n",
    "                de_features = subject_features[f'de_LDS_{video_index}']\n",
    "                # Flatten the frequency band and EEG channel dimensions.\n",
    "                de_features = de_features.reshape(de_features.shape[0], -1)\n",
    "                # Shape: (sequence length, input size (5 * 62))\n",
    "                self.features.append(de_features)\n",
    "        labels = pd.read_excel(\n",
    "            DATASET_PATH / 'emotion_label_and_stimuli_order.xlsx', header=None,\n",
    "            usecols='B:U', skiprows=lambda row_index: row_index % 2 == 0\n",
    "        )\n",
    "        labels = labels.values.flatten().tolist()\n",
    "        labels = [EMOTIONS[label] for label in labels]\n",
    "        labels = labels * 20\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[np.ndarray, int]:\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a7f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "(array([[26.35287574, 27.35400046, 27.24510979, ..., 15.19771485,\n",
      "        15.32183227, 15.35056856],\n",
      "       [26.35106677, 27.35408187, 27.2440835 , ..., 15.19913513,\n",
      "        15.32243371, 15.35182064],\n",
      "       [26.3545593 , 27.35613658, 27.24467413, ..., 15.20061924,\n",
      "        15.32347101, 15.35357   ],\n",
      "       ...,\n",
      "       [26.40828782, 27.35588376, 27.25665373, ..., 15.2133756 ,\n",
      "        15.33720278, 15.38302023],\n",
      "       [26.40705078, 27.35320526, 27.25430671, ..., 15.21125846,\n",
      "        15.33683886, 15.38170078],\n",
      "       [26.40681781, 27.34966302, 27.25159395, ..., 15.21022576,\n",
      "        15.33692793, 15.3811932 ]], shape=(15, 310)), 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = SeedDataset()\n",
    "row_count = len(dataset)\n",
    "print(row_count)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b55422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 13, Max: 87, Mean: 43.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_lens = []\n",
    "for i in range(len(dataset)):\n",
    "    seq = dataset[i][0]\n",
    "    seq_lens.append(seq.shape[0])\n",
    "plt.hist(seq_lens, bins=20)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sequence Lengths in SEED')\n",
    "plt.show()\n",
    "print(f\"Min: {min(seq_lens)}, Max: {max(seq_lens)}, Mean: {sum(seq_lens)/len(seq_lens):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca2aaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_encode(features, num_steps=20):\n",
    "    \"\"\"\n",
    "    features: torch.Tensor, shape (sequence_len, feature_dim)\n",
    "    Returns: torch.Tensor, shape (num_steps, sequence_len, feature_dim)\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1] for firing rates\n",
    "    features_norm = features / features.max()\n",
    "    # Poisson encoding: at each time step, a spike occurs if rand < rate\n",
    "    rand = torch.rand((num_steps, *features_norm.shape), device=features.device)\n",
    "    spikes = (rand < features_norm).float()\n",
    "    return spikes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "923a30b6-68bd-4e7c-b82a-a4232d346ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loaders() -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    dataset = SeedDataset()\n",
    "    row_count = len(dataset)\n",
    "    train_indices = list(range(0, int(TRAIN_SPLIT * row_count)))\n",
    "    validation_indices = list(range(\n",
    "        int(TRAIN_SPLIT * row_count),\n",
    "        int((TRAIN_SPLIT + VALIDATION_SPLIT) * row_count)))\n",
    "    test_indices = list(range(\n",
    "        int((TRAIN_SPLIT + VALIDATION_SPLIT) * row_count), row_count))\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    validation_dataset = Subset(dataset, validation_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    def collate_fn(batch, num_steps=20, fixed_seq_len=70): \n",
    "        feature_dim = batch[0][0].shape[1]\n",
    "        encoded_sequences = []\n",
    "        for seq, _ in batch:\n",
    "            seq_tensor = torch.tensor(seq, dtype=torch.float)\n",
    "            # Truncate if too long\n",
    "            if seq_tensor.shape[0] > fixed_seq_len:\n",
    "                seq_tensor = seq_tensor[:fixed_seq_len, :]\n",
    "            # Pad if too short\n",
    "            pad_len = fixed_seq_len - seq_tensor.shape[0]\n",
    "            if pad_len > 0:\n",
    "                pad = torch.zeros((pad_len, feature_dim), dtype=seq_tensor.dtype, device=seq_tensor.device)\n",
    "                seq_tensor = torch.cat([seq_tensor, pad], dim=0)\n",
    "            # Poisson encode\n",
    "            encoded = poisson_encode(seq_tensor, num_steps=num_steps)\n",
    "            encoded_sequences.append(encoded)\n",
    "        batch_spikes = torch.stack(encoded_sequences)\n",
    "        labels = torch.tensor([label for _, label in batch], dtype=torch.long)\n",
    "        return batch_spikes, labels\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, NUM_STEPS, FIXED_SEQ_LEN)\n",
    "    )\n",
    "    validation_loader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, NUM_STEPS, FIXED_SEQ_LEN)\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, NUM_STEPS, FIXED_SEQ_LEN)\n",
    "    )\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12696027-e34f-47b8-bf11-1f355e68106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader, test_loader = get_data_loaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29b35fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader output shapes:\n",
      "  batch_spikes: torch.Size([64, 10, 70, 310])\n",
      "  labels: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_spikes, labels = next(iter(train_loader))  # One batch\n",
    "\n",
    "print(f\"Dataloader output shapes:\")\n",
    "print(f\"  batch_spikes: {batch_spikes.shape}\")\n",
    "print(f\"  labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6eea485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakySurrogate(nn.Module):\n",
    "    def __init__(self, beta, z=2, threshold=0.8):\n",
    "        super(LeakySurrogate, self).__init__()\n",
    "\n",
    "        # initialize decay rate beta and threshold\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "        self.spike_op = self.SpikeOperator.apply\n",
    "        self.z = z\n",
    "        self.mem = None\n",
    "\n",
    "    # the forward function is called each time we call Leaky\n",
    "    def forward(self, input_):\n",
    "        spk = self.spike_op(self.mem - self.threshold, self.z)  # call the Heaviside function\n",
    "        reset = (spk * self.threshold).detach() # removes spike_op gradient from reset\n",
    "        self.mem = self.beta * self.mem + input_ - reset\n",
    "        return spk\n",
    "\n",
    "    # forward pass: Heaviside function\n",
    "    @staticmethod\n",
    "    class SpikeOperator(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, z):\n",
    "            \"\"\"\n",
    "            In the forward pass we compute a step function of the input Tensor\n",
    "            and return it. ctx is a context object that we use to stash information which\n",
    "            we need to later backpropagate our error signals. To achieve this we use the\n",
    "            ctx.save_for_backward method.\n",
    "            \"\"\"\n",
    "            ctx.save_for_backward(input)\n",
    "            ctx.z = z\n",
    "            spk = torch.zeros_like(input)\n",
    "            spk[input > 0] = 1.0\n",
    "            return spk\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            In the backward pass we receive a Tensor we need to compute the\n",
    "            surrogate gradient of the loss with respect to the input.\n",
    "            Here we use the fast Sigmoid function with z = 1.\n",
    "            \"\"\"\n",
    "            input, = ctx.saved_tensors\n",
    "            z = ctx.z\n",
    "            grad_input = grad_output.clone()\n",
    "            grad = (\n",
    "                grad_input\n",
    "                * z * torch.exp(-z * input)\n",
    "                / (torch.exp(-z * input) + 1) ** 2\n",
    "            )\n",
    "            return grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7880b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, T, beta=0.8, z=1, threshold=1.0):\n",
    "        super(SNN, self).__init__()\n",
    "        self.T = T\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 1st fully-connected layer\n",
    "        self.fc1 = nn.Linear(FIXED_SEQ_LEN * 310, 64) # feature_dim = 310\n",
    "        self.lif1 = LeakySurrogate(beta=beta, z=z, threshold=threshold)\n",
    "        # 2nd fully-connected layer\n",
    "        self.fc2 = nn.Linear(64, 7)\n",
    "        # output layer neurons, whose firing rate will be served as the final prediction\n",
    "        self.lif2 = LeakySurrogate(beta=beta, z=z, threshold=threshold)\n",
    "\n",
    "    def init_mem(self, batch_size, feature_num, device):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(batch_size, feature_num, device=device))\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, input_):\n",
    "        self.lif1.mem = self.init_mem(input_.shape[1], 64, input_.device)\n",
    "        self.lif2.mem = self.init_mem(input_.shape[1], 7, input_.device)\n",
    "\n",
    "        output_spikes = 0\n",
    "        for t in range(self.T):\n",
    "            x = input_[t]\n",
    "            x = self.flatten(x)\n",
    "            x = self.fc1(x)\n",
    "            spk1 = self.lif1(x)\n",
    "            x = self.fc2(spk1)\n",
    "            spk2 = self.lif2(x)\n",
    "            output_spikes = output_spikes + spk2\n",
    "\n",
    "        return output_spikes / self.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec55ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7887793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# model = SNN(T=20).to(device)\n",
    "# batch_spikes = batch_spikes.to(device)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "# batch_size, T, seq_len, feat_dim = batch_spikes.shape\n",
    "# input_dim = seq_len * feat_dim  # 82 * 310 = 25420\n",
    "# batch_spikes_reshaped = batch_spikes.view(batch_size, T, input_dim)\n",
    "# batch_spikes_reshaped = batch_spikes_reshaped.permute(1, 0, 2)  # (T, batch, input_dim)\n",
    "\n",
    "# print(\"Model input shape:\", batch_spikes_reshaped.shape)\n",
    "# # Model input shape: torch.Size([20, 128, 25420])\n",
    "# outputs = model(batch_spikes_reshaped)\n",
    "# print(\"Model output shape:\", outputs.shape)\n",
    "# # Should be (128, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05eb1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, data_loader: DataLoader,\n",
    "                optimizer: optim.Optimizer, criterion: nn.Module,\n",
    "                device: torch.device) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    num_classes = 7  # or make this an argument\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_spikes, labels = batch\n",
    "        batch_spikes = batch_spikes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size, T, seq_len, feat_dim = batch_spikes.shape\n",
    "        input_dim = seq_len * feat_dim\n",
    "        batch_spikes = batch_spikes.view(batch_size, T, input_dim)\n",
    "        batch_spikes = batch_spikes.permute(1, 0, 2)\n",
    "        outputs = model(batch_spikes)\n",
    "\n",
    "        # Pick the correct target type for loss\n",
    "        if isinstance(criterion, torch.nn.MSELoss):\n",
    "            labels_onehot = torch.zeros(labels.size(0), num_classes, device=labels.device)\n",
    "            labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "            loss = criterion(outputs, labels_onehot)\n",
    "        else:  # CrossEntropyLoss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRADIENT_CLIPPING_MAX_NORM)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.shape[0]\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        total_count += labels.shape[0]\n",
    "        correct_count += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    accuracy = correct_count / total_count\n",
    "    return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce94538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, data_loader: DataLoader, criterion: nn.Module,\n",
    "             device: torch.device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_spikes, labels = batch\n",
    "            batch_spikes = batch_spikes.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch_size, T, seq_len, feat_dim = batch_spikes.shape\n",
    "            input_dim = seq_len * feat_dim\n",
    "            batch_spikes = batch_spikes.view(batch_size, T, input_dim)\n",
    "            batch_spikes = batch_spikes.permute(1, 0, 2)\n",
    "            outputs = model(batch_spikes)\n",
    "\n",
    "            if isinstance(criterion, torch.nn.MSELoss):\n",
    "                labels_onehot = torch.zeros(labels.size(0), 7, device=labels.device)\n",
    "                labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "                loss = criterion(outputs, labels_onehot)\n",
    "            else:  # CrossEntropyLoss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.shape[0]\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            total_count += labels.shape[0]\n",
    "            correct_count += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    accuracy = correct_count / total_count\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d80c566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> tuple[list[float], list[float], list[float], list[float]]:\n",
    "    train_loader, validation_loader, test_loader = get_data_loaders()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}.')\n",
    "    model = SNN(T=NUM_STEPS, beta=0.8, z=2, threshold=0.8).to(device)  # Set T to match Poisson encoding steps\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    best_validation_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_without_improvement = 0\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        train_loss, train_accuracy = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device)\n",
    "        validation_loss, validation_accuracy = evaluate(\n",
    "            model, validation_loader, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        if epoch == 0 or (epoch + 1) % PRINT_FREQUENCY_EPOCHS == 0:\n",
    "            print(f'Epoch {epoch + 1}: '\n",
    "                  f'Train loss: {train_loss:.4f}, '\n",
    "                  f'Train accuracy: {train_accuracy:.4f}, '\n",
    "                  f'Validation loss: {validation_loss:.4f}, '\n",
    "                  f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            best_model = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f'Stopping because validation loss did not improve for '\n",
    "                  f'{EARLY_STOPPING_PATIENCE} epochs. Best validation loss: '\n",
    "                  f'{best_validation_loss:.4f}')\n",
    "            break\n",
    "    model.load_state_dict(best_model)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), 'snn_classifier.pt')\n",
    "    print('Model saved as `snn_classifier.pt`.')\n",
    "    return (train_losses, train_accuracies, validation_losses, validation_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6ef4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses: list[float],\n",
    "                         train_accuracies: list[float],\n",
    "                         validation_losses: list[float],\n",
    "                         validation_accuracies: list[float]):\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_losses, label='Train loss')\n",
    "    plt.plot(epochs, validation_losses, label='Validation loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss.png')\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_accuracies, label='Train accuracy')\n",
    "    plt.plot(epochs, validation_accuracies, label='Validation accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('accuracy.png')\n",
    "    print('Loss and accuracy curves saved as `loss.png` and `accuracy.png`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cda5f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n",
      "Epoch 1: Train loss: 1.8970, Train accuracy: 0.2555, Validation loss: 1.8615, Validation accuracy: 0.3187\n",
      "Epoch 10: Train loss: 1.9466, Train accuracy: 0.1234, Validation loss: 1.9434, Validation accuracy: 0.1187\n",
      "Epoch 20: Train loss: 1.9462, Train accuracy: 0.1203, Validation loss: 1.9458, Validation accuracy: 0.1187\n",
      "Epoch 30: Train loss: 1.9453, Train accuracy: 0.1156, Validation loss: 1.9453, Validation accuracy: 0.1313\n",
      "Epoch 40: Train loss: 1.9458, Train accuracy: 0.1195, Validation loss: 1.9463, Validation accuracy: 0.1250\n",
      "Epoch 50: Train loss: 1.9465, Train accuracy: 0.1180, Validation loss: 1.9439, Validation accuracy: 0.1187\n",
      "Epoch 60: Train loss: 1.9464, Train accuracy: 0.1117, Validation loss: 1.9474, Validation accuracy: 0.1062\n",
      "Epoch 70: Train loss: 1.9477, Train accuracy: 0.1031, Validation loss: 1.9460, Validation accuracy: 0.1313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m     plot_training_curves(train_losses, train_accuracies, validation_losses,\n\u001b[0;32m      5\u001b[0m                          validation_accuracies)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      2\u001b[0m     (train_losses, train_accuracies, validation_losses,\n\u001b[1;32m----> 3\u001b[0m      validation_accuracies) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     plot_training_curves(train_losses, train_accuracies, validation_losses,\n\u001b[0;32m      5\u001b[0m                          validation_accuracies)\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m epochs_without_improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_EPOCHS):\n\u001b[1;32m---> 17\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     validation_loss, validation_accuracy \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m     20\u001b[0m         model, validation_loader, criterion, device)\n\u001b[0;32m     21\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m batch_spikes \u001b[38;5;241m=\u001b[39m batch_spikes\u001b[38;5;241m.\u001b[39mview(batch_size, T, input_dim)\n\u001b[0;32m     18\u001b[0m batch_spikes \u001b[38;5;241m=\u001b[39m batch_spikes\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_spikes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Pick the correct target type for loss\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(criterion, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss):\n",
      "File \u001b[1;32mc:\\Users\\91486\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91486\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 29\u001b[0m, in \u001b[0;36mSNN.forward\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m     27\u001b[0m     spk1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(x)\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk1)\n\u001b[1;32m---> 29\u001b[0m     spk2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlif2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     output_spikes \u001b[38;5;241m=\u001b[39m output_spikes \u001b[38;5;241m+\u001b[39m spk2\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_spikes \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\91486\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91486\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mLeakySurrogate.forward\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[1;32m---> 14\u001b[0m     spk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# call the Heaviside function\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     reset \u001b[38;5;241m=\u001b[39m (spk \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold)\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;66;03m# removes spike_op gradient from reset\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m+\u001b[39m input_ \u001b[38;5;241m-\u001b[39m reset\n",
      "File \u001b[1;32mc:\\Users\\91486\\miniconda3\\envs\\dl_proj\\lib\\site-packages\\torch\\autograd\\function.py:559\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the behavior for this autograd.Function underneath :func:`torch.vmap`.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \n\u001b[0;32m    530\u001b[0m \u001b[38;5;124;03m    For a :func:`torch.autograd.Function` to support\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    Please see :ref:`func-autograd-function` for more details.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use autograd.Function with vmap, you must either override the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmap staticmethod or set generate_vmap_rule=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    557\u001b[0m     )\n\u001b[1;32m--> 559\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind_default_args\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    562\u001b[0m         signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    (train_losses, train_accuracies, validation_losses,\n",
    "     validation_accuracies) = train()\n",
    "    plot_training_curves(train_losses, train_accuracies, validation_losses,\n",
    "                         validation_accuracies)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ded54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
