{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c49a2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fe9dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e4c2c",
   "metadata": {},
   "source": [
    "# data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d33df7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/my_horizon_data_all.csv\", dtype={\"subject\": str})\n",
    "\n",
    "# files = [\n",
    "#     \"data/my_horizon_data.csv\",\n",
    "#     \"data/my_horizon_data_0919.csv\",\n",
    "# ]\n",
    "# df = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b9f69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(5000)  # For quicker testing !!!!! REMEBER TO RMOVE IT!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d72679ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subjects: 1384\n"
     ]
    }
   ],
   "source": [
    "num_subjects = df[\"subject\"].nunique()\n",
    "print(\"Number of unique subjects:\", num_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66ba6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # boundary flag\n",
    "# df['boundary'] = (df['block'] != df.groupby('subject')['block'].shift(1)).astype(int)\n",
    "# print(df[['subject', 'block', 'boundary']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7699168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seq_features = [\n",
    "    'r0', 'c0',\n",
    "    'r1', 'c1',\n",
    "    'r2', 'c2',\n",
    "    'r3', 'c3',\n",
    "]\n",
    "static_features = ['gameLength', 'uc']\n",
    "\n",
    "target = 'c4'\n",
    "\n",
    "subj_onehot = pd.get_dummies(df['subject'], prefix='subj')\n",
    "df_static = pd.concat([df[static_features], subj_onehot], axis=1)\n",
    "\n",
    "X_seq = df[seq_features]\n",
    "X_static = df_static\n",
    "y = df[target]\n",
    "\n",
    "# split based on subjects\n",
    "subjects = df['subject'].unique()\n",
    "train_subj, test_subj = train_test_split(subjects, test_size=0.2, random_state=42)\n",
    "\n",
    "train_mask = df['subject'].isin(train_subj)\n",
    "test_mask  = df['subject'].isin(test_subj)\n",
    "\n",
    "X_seq_train = df.loc[train_mask, seq_features]\n",
    "X_seq_test  = df.loc[test_mask, seq_features]\n",
    "\n",
    "X_static_train = df_static.loc[train_mask]\n",
    "X_static_test  = df_static.loc[test_mask]\n",
    "\n",
    "y_train = y[train_mask]\n",
    "y_test  = y[test_mask]\n",
    "\n",
    "\n",
    "# split based on original data frame\n",
    "h1_mask = X_static_test['gameLength'] == 1\n",
    "h6_mask = X_static_test['gameLength'] == 6\n",
    "X_static_test_raw = X_static_test.copy()\n",
    "\n",
    "\n",
    "# separate numeric static vs one-hot\n",
    "onehot_cols = [c for c in X_static.columns if c.startswith('subj_')]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_seq_train = scaler.fit_transform(X_seq_train)\n",
    "X_seq_test = scaler.transform(X_seq_test)\n",
    "\n",
    "X_static_train_num = scaler.fit_transform(X_static_train[static_features])\n",
    "X_static_test_num  = scaler.transform(X_static_test[static_features])\n",
    "\n",
    "# keep one-hot untouched\n",
    "X_static_train_oh = X_static_train[onehot_cols].to_numpy()\n",
    "X_static_test_oh  = X_static_test[onehot_cols].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a27c6c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_LEN: 2\n",
      "STATIC_LEN: 1386\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = seq_features.__len__() // 4 \n",
    "print(f\"SEQ_LEN: {SEQ_LEN}\")\n",
    "STATIC_LEN = X_static_train.shape[1]\n",
    "print(f\"STATIC_LEN: {STATIC_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7468e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['r0' 'c0']\n",
      "  ['r1' 'c1']\n",
      "  ['r2' 'c2']\n",
      "  ['r3' 'c3']]]\n"
     ]
    }
   ],
   "source": [
    "feature_order_test = np.array(seq_features).reshape(-1, 4, SEQ_LEN)\n",
    "print(feature_order_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9547fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 9.99078375e-01  1.37538975e+00]]\n",
      "\n",
      " [[ 9.99078375e-01 -6.74679601e-04]]\n",
      "\n",
      " [[ 9.99078375e-01 -6.74679601e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.00092248e+00 -6.74679601e-04]]\n",
      "\n",
      " [[-1.00092248e+00 -1.37673911e+00]]\n",
      "\n",
      " [[-1.00092248e+00  1.37538975e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_static_train_num[:, np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e0e6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_train = X_seq_train.reshape(-1, 4, SEQ_LEN)\n",
    "X_seq_test  = X_seq_test.reshape(-1, 4, SEQ_LEN)\n",
    "\n",
    "\n",
    "# Repeat numeric static info across 4 time steps, and append to sequential inputs\n",
    "X_static_train_num_rep = np.repeat(X_static_train_num[:, np.newaxis, :], 4, axis=1)\n",
    "X_static_test_num_rep  = np.repeat(X_static_test_num[:, np.newaxis, :], 4, axis=1)\n",
    "\n",
    "X_seq_train = np.concatenate([X_seq_train, X_static_train_num_rep], axis=2)\n",
    "X_seq_test  = np.concatenate([X_seq_test, X_static_test_num_rep], axis=2)\n",
    "\n",
    "# Final static input = subject one-hot only\n",
    "X_static_train = X_static_train_oh\n",
    "X_static_test  = X_static_test_oh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4137c6",
   "metadata": {},
   "source": [
    "convert to tensor for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "129e6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_seq_train_tensor = torch.tensor(X_seq_train, dtype=torch.float32) # sklearn output float64, doesn't work with torch\n",
    "X_seq_test_tensor = torch.tensor(X_seq_test, dtype=torch.float32) \n",
    "\n",
    "X_static_train_tensor = torch.tensor(X_static_train, dtype=torch.float32) \n",
    "X_static_test_tensor = torch.tensor(X_static_test, dtype=torch.float32) \n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long) # pandas series to tensor\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2baf858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique block counts: [160 128 121 120 125 320 224 192 134 129 147 100  82 106 123 163 157 149\n",
      "  90 137 118 102  92  94  51  75  98  74  25  85  89 119  93  62  69 104\n",
      "  86  88  80  76  59 109  53  83 113  96 101 103  72  68  99  58  95  78\n",
      "  61 107  35  87  79  14]\n"
     ]
    }
   ],
   "source": [
    "block_counts = df.groupby('subject')['block'].nunique()\n",
    "print(\"Unique block counts:\", block_counts.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddaabfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(TensorDataset(X_seq_train_tensor, X_static_train_tensor, y_train_tensor), batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(TensorDataset(X_seq_test_tensor, X_static_test_tensor, y_test_tensor), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b41ca303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index mapping *within* train/test subsets\n",
    "train_subjects_series = df.loc[train_mask, 'subject'].reset_index(drop=True)\n",
    "test_subjects_series  = df.loc[test_mask, 'subject'].reset_index(drop=True)\n",
    "\n",
    "# Now build mapping of subject -> tensor-relative indices\n",
    "train_subject_indices = {\n",
    "    subj: np.where(train_subjects_series == subj)[0]\n",
    "    for subj in train_subj\n",
    "}\n",
    "test_subject_indices = {\n",
    "    subj: np.where(test_subjects_series == subj)[0]\n",
    "    for subj in test_subj\n",
    "}\n",
    "\n",
    "# Make per-subject tensors (now indices are valid)\n",
    "train_groups = [\n",
    "    (subj,\n",
    "     X_seq_train_tensor[idx],\n",
    "     X_static_train_tensor[idx],\n",
    "     y_train_tensor[idx])\n",
    "    for subj, idx in train_subject_indices.items()\n",
    "]\n",
    "\n",
    "test_groups = [\n",
    "    (subj,\n",
    "     X_seq_test_tensor[idx],\n",
    "     X_static_test_tensor[idx],\n",
    "     y_test_tensor[idx])\n",
    "    for subj, idx in test_subject_indices.items()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53aba66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectLoader:\n",
    "    def __init__(self, groups):\n",
    "        self.groups = groups\n",
    "    def __iter__(self):\n",
    "        for subj, X_seq, X_static, y in self.groups:\n",
    "            yield subj, X_seq, X_static, y\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "train_loader = SubjectLoader(train_groups)\n",
    "test_loader  = SubjectLoader(test_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acfbc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the scaled tensor for h1 h6\n",
    "# convert pandas series -> numpy array -> torch BoolTensor\n",
    "h1_idx = torch.tensor(h1_mask.to_numpy(), dtype=torch.bool)\n",
    "h6_idx = torch.tensor(h6_mask.to_numpy(), dtype=torch.bool)\n",
    "\n",
    "X_seq_test_h1 = X_seq_test_tensor[h1_idx]\n",
    "X_seq_test_h6 = X_seq_test_tensor[h6_idx]\n",
    "\n",
    "X_static_test_h1 = X_static_test_tensor[h1_idx]\n",
    "X_static_test_h6 = X_static_test_tensor[h6_idx]\n",
    "\n",
    "y_test_h1 = y_test_tensor[h1_idx]\n",
    "y_test_h6 = y_test_tensor[h6_idx]\n",
    "\n",
    "test_groups_h1 = [(\"H1\", X_seq_test_h1, X_static_test_h1, y_test_h1)]\n",
    "test_groups_h6 = [(\"H6\", X_seq_test_h6, X_static_test_h6, y_test_h6)]\n",
    "\n",
    "test_loader_h1 = SubjectLoader(test_groups_h1)\n",
    "test_loader_h6 = SubjectLoader(test_groups_h6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4593b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c78745",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "891d60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNbeta(nn.Module):\n",
    "    def __init__(self, seq_input_size=SEQ_LEN, hidden_size=64, output_size=2):\n",
    "        super(RNNbeta, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(seq_input_size, hidden_size, batch_first=True)\n",
    "        self.logit_beta = nn.Parameter(torch.tensor(0.0))  # initialize logit(0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32), # combine static inputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size), # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_x, h=None):\n",
    "        if h is not None:\n",
    "            beta = torch.sigmoid(self.logit_beta)  # constrain between 0 and 1\n",
    "            h = beta * h.detach()\n",
    "        gru_out, h_n = self.rnn(seq_x, h)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        \n",
    "        output = self.fc(h_n)\n",
    "        return output, h_n.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9534dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbeta(nn.Module):\n",
    "    def __init__(self, seq_input_size=SEQ_LEN, hidden_size=64, output_size=2):\n",
    "        super(LSTMbeta, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(seq_input_size, hidden_size, batch_first=True)\n",
    "        self.logit_beta = nn.Parameter(torch.tensor(0.0))  # logit(0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_x, hc=None):\n",
    "        # hc may be None (first game) or a tuple (h, c)\n",
    "        if hc is not None:\n",
    "            h, c = hc\n",
    "            beta = torch.sigmoid(self.logit_beta)\n",
    "            # scale both hidden and cell states\n",
    "            h = beta * h\n",
    "            c = beta * c\n",
    "            hc = (h, c)\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(seq_x, hc)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        output = self.fc(h_n)\n",
    "        return output, (h_n.unsqueeze(0), c_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15347c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, seq_input_size=SEQ_LEN, hidden_size=64, output_size=2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(seq_input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32), # combine static inputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size), # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_x, h=None):\n",
    "        gru_out, h_n = self.gru(seq_x, h)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        \n",
    "        output = self.fc(h_n)\n",
    "        return output, h_n.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef98b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUbeta(nn.Module):\n",
    "    def __init__(self, seq_input_size=SEQ_LEN, hidden_size=64, output_size=2):\n",
    "        super(GRUbeta, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(seq_input_size, hidden_size, batch_first=True)\n",
    "        self.logit_beta = nn.Parameter(torch.tensor(0.0))  # initialize logit(0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32), # combine static inputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size), # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_x, h=None):\n",
    "        if h is not None:\n",
    "            beta = torch.sigmoid(self.logit_beta)  # constrain between 0 and 1\n",
    "            h = beta * h.detach()\n",
    "        gru_out, h_n = self.gru(seq_x, h)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        \n",
    "        output = self.fc(h_n)\n",
    "        return output, h_n.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cd76ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGRUbeta(nn.Module):\n",
    "    def __init__(self, seq_input_size=SEQ_LEN, hidden_size=2, output_size=2):\n",
    "        super(TinyGRUbeta, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(seq_input_size, hidden_size, batch_first=True)\n",
    "        self.logit_beta = nn.Parameter(torch.tensor(0.0))  # initialize logit(0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32), # combine static inputs\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size), # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_x, h=None):\n",
    "        if h is not None:\n",
    "            beta = torch.sigmoid(self.logit_beta)  # constrain between 0 and 1\n",
    "            h = beta * h.detach()\n",
    "        gru_out, h_n = self.gru(seq_x, h)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        \n",
    "        output = self.fc(h_n)\n",
    "        return output, h_n.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd32b8",
   "metadata": {},
   "source": [
    "# train / eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "139239ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0  # total number of games (not batches)\n",
    "\n",
    "    BATCH_GAMES = 32  # tune this as needed\n",
    "\n",
    "    for subj_id, seqs, statics, labels in train_loader:\n",
    "        h = None\n",
    "        subj_loss = 0.0 \n",
    "\n",
    "        for i in range(0, len(seqs), BATCH_GAMES):\n",
    "            x_batch = seqs[i:i+BATCH_GAMES].to(device)\n",
    "            y_batch = labels[i:i+BATCH_GAMES].to(device)\n",
    "\n",
    "            # --- prepare hidden state ---\n",
    "            if h is not None:\n",
    "                if isinstance(h, tuple):  # LSTM (h, c)\n",
    "                    h0, c0 = h\n",
    "                    if h0.size(1) != len(x_batch):\n",
    "                        h = (\n",
    "                            torch.zeros(1, len(x_batch), model.hidden_size, device=device),\n",
    "                            torch.zeros(1, len(x_batch), model.hidden_size, device=device),\n",
    "                        )\n",
    "                else:  # GRU\n",
    "                    if h.size(1) != len(x_batch):\n",
    "                        h = torch.zeros(1, len(x_batch), model.hidden_size, device=device)\n",
    "\n",
    "            # --- forward pass ---\n",
    "            preds, h = model(x_batch, h)\n",
    "            loss = criterion(preds, y_batch)\n",
    "\n",
    "            # --- backward pass ---\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- detach hidden state ---\n",
    "            if isinstance(h, tuple):\n",
    "                h = (h[0].detach(), h[1].detach())\n",
    "            else:\n",
    "                h = h.detach()\n",
    "\n",
    "            # --- record stats ---\n",
    "            subj_loss += loss.item() * len(x_batch)\n",
    "            correct += (preds.argmax(dim=1) == y_batch).sum().item()\n",
    "            total += len(x_batch)\n",
    "\n",
    "        total_loss += subj_loss\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for subj_id, seqs, statics, labels in test_loader:\n",
    "            h = None  # reset per subject\n",
    "\n",
    "            seqs = seqs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            for i in range(len(seqs)):\n",
    "                x_seq = seqs[i].unsqueeze(0)\n",
    "                y_true = labels[i].unsqueeze(0)\n",
    "\n",
    "                preds, h = model(x_seq, h)\n",
    "                loss = criterion(preds, y_true)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                correct += (preds.argmax(dim=1) == y_true).sum().item()\n",
    "                total += 1\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bd35ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    train_loss_prog, train_acc_prog = [], []\n",
    "    test_loss_prog, test_acc_prog = [], []\n",
    "    test_acc_h1_prog, test_loss_h1_prog = [], []\n",
    "    test_acc_h6_prog, test_loss_h6_prog = [], []\n",
    "\n",
    "    epochs_without_improvement = 0\n",
    "    best_loss = float('inf')\n",
    "    PATIENCE = 5\n",
    "    final_epoch = epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ----- TRAIN -----\n",
    "        train_acc, train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # ----- EVALUATE -----\n",
    "        test_acc, test_loss = test(model, test_loader, criterion, device)\n",
    "        test_acc_h1, test_loss_h1 = test(model, test_loader_h1, criterion, device)\n",
    "        test_acc_h6, test_loss_h6 = test(model, test_loader_h6, criterion, device)\n",
    "\n",
    "        # ----- LOG -----\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            # If your model returns β (GRUβ)\n",
    "            if hasattr(model, \"logit_beta\"):\n",
    "                beta_val = torch.sigmoid(model.logit_beta).item()\n",
    "                print(f\"Epoch {epoch+1}: Loss: {test_loss:.4f} | overall: {test_acc:.4f} | H1 {test_acc_h1:.4f} | H6 {test_acc_h6:.4f} | β={beta_val:.3f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: Loss: {test_loss:.4f} | overall: {test_acc:.4f} | H1 {test_acc_h1:.4f} | H6 {test_acc_h6:.4f}\")\n",
    "\n",
    "        # ----- STORE PROGRESS -----\n",
    "        train_acc_prog.append(train_acc)\n",
    "        train_loss_prog.append(train_loss)\n",
    "\n",
    "        test_loss_prog.append(test_loss)\n",
    "        test_acc_prog.append(test_acc)\n",
    "\n",
    "        test_acc_h1_prog.append(test_acc_h1)\n",
    "        test_loss_h1_prog.append(test_loss_h1)\n",
    "        test_acc_h6_prog.append(test_acc_h6)\n",
    "        test_loss_h6_prog.append(test_loss_h6)\n",
    "\n",
    "        # ----- EARLY STOPPING -----\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement > PATIENCE:\n",
    "            print(f\"Early stopping triggered: epoch {epoch+1}, best_loss {best_loss:.4f}\")\n",
    "            final_epoch = epoch + 1\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"train_loss_prog\": train_loss_prog,\n",
    "        \"train_acc_prog\": train_acc_prog,\n",
    "        \"test_loss_prog\": test_loss_prog,\n",
    "        \"test_acc_prog\": test_acc_prog,\n",
    "        \"test_acc_h1_prog\": test_acc_h1_prog,\n",
    "        \"test_loss_h1_prog\": test_loss_h1_prog,\n",
    "        \"test_acc_h6_prog\": test_acc_h6_prog,\n",
    "        \"test_loss_h6_prog\": test_loss_h6_prog,\n",
    "        \"final_epoch\": final_epoch\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f45961",
   "metadata": {},
   "source": [
    "# running experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7a02a",
   "metadata": {},
   "source": [
    "## multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3395551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"LSTM\": LSTMbeta().to(device),\n",
    "    # \"RNN\": RNNbeta().to(device),\n",
    "    # \"GRU\": GRU().to(device),\n",
    "    # \"GRUbeta\": GRUbeta().to(device),\n",
    "    # \"TinyGRU\": TinyGRUbeta().to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b82e7321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: LSTM\n",
      "Epoch 1: Loss: 0.5141 | overall: 0.7683 | H1 0.8094 | H6 0.7283 | β=0.292\n",
      "Epoch 10: Loss: 0.4987 | overall: 0.7758 | H1 0.8156 | H6 0.7365 | β=0.099\n",
      "Epoch 20: Loss: 0.4995 | overall: 0.7748 | H1 0.8131 | H6 0.7368 | β=0.144\n",
      "Early stopping triggered: epoch 21, best_loss 0.4982\n",
      "LSTM: final learned β = 0.149\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for model_name, model in model_dict.items():\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    history = train_and_evaluate(model, train_loader, criterion, optimizer, device, epochs=epochs)\n",
    "    if hasattr(model, \"logit_beta\"):\n",
    "        beta_val = torch.sigmoid(model.logit_beta).item()\n",
    "        print(f\"{model_name}: final learned β = {beta_val:.3f}\")\n",
    "    model_dict[model_name] = {\n",
    "        \"model\": model,\n",
    "        **history   # unpack dictionary contents into this model’s record\n",
    "    }\n",
    "    # torch.save(model.state_dict(), f\"model_weights_crossgame_{model_name}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c6b9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_history = {}\n",
    "\n",
    "# for model_name, content in model_dict.items():\n",
    "#     # content = {\"model\": <model_object>, \"train_acc\": ..., \"test_acc\": ..., ...}\n",
    "#     filtered = {k: v for k, v in content.items() if k != \"model\"}\n",
    "#     dump_history[model_name] = filtered\n",
    "# # store the outputs \n",
    "# import json\n",
    "# with open(\"output_cross_games.json\", \"w\") as f:\n",
    "#     json.dump(dump_history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
